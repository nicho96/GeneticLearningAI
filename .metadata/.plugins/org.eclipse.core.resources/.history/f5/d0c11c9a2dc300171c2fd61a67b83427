package ca.nicho.neuralnet;

import java.util.ArrayList;
import java.util.Collections;
import java.util.Random;

public class DefaultNEAT extends NEAT {

	public static final double DEVIATION_THRESHOLD = 0;
	
	private int speciesCapacity = 50;
	private Random random = new Random();
	private SimulateDelegate simulationDelegate;
	
	@Override
	public void crossoverGeneration() {
		int bredCount = 0;
		while(bredCount < speciesCapacity){
			NeuralNetwork nn1 = networks.get(random.nextInt(networks.size()));
			NeuralNetwork nn2 = null;
			while(nn2 == null || nn2 == nn1){
				nn2 = networks.get(random.nextInt(networks.size()));
			}
			NeuralNetwork child = this.crossover(nn1, nn2);
			double dev = Math.min(getDeviation(child, nn1), getDeviation(child, nn2));
			if(dev > DEVIATION_THRESHOLD){
				this.networks.add(child);
			}
			bredCount++;
		}
	}

	@Override
	public void simulateGeneration() {
		for(NeuralNetwork nn : networks)
			simulationDelegate.simulateNetwork(nn);
	}

	@Override
	public void mutate(NeuralNetwork nn) {
		
		for(int j = 0; j < nn.axons.size() * 0.02 + 1; j++){
			while(!mutateNetwork(nn));
		}
		
	}

	@Override
	public NeuralNetwork crossover(NeuralNetwork fittest, NeuralNetwork other) {
		//Ensures variable fittest is indeed the fittest
			if(fittest.score < other.score){
				NeuralNetwork tmp = fittest;
				fittest = other;
				other = tmp;
			}
			
			boolean equal = fittest.score == other.score;
			
			NeuralNetwork clone = new NeuralNetwork(fittest);
			
			//Mix any new innovations from the other network
			long max = Math.max(fittest.maxInnovation, other.maxInnovation);
			for(long i = 0; i < max; i++){
				if(fittest.axonsMap.containsKey(i) && other.axonsMap.containsKey(i)){
					
					if(equal){
						clone.axonsMap.get(i).weight = (random.nextBoolean()) ? fittest.axonsMap.get(i).weight : other.axonsMap.get(i).weight;
					}
					
					//Weight is inherited by the most fit network, but whether or not it's enabled can be inherited by either (by chance)
					clone.axonsMap.get(i).enabled = (random.nextBoolean()) ? fittest.axonsMap.get(i).enabled : other.axonsMap.get(i).enabled;
					
				}else if(other.axonsMap.containsKey(i)){
					Axon gene = other.axonsMap.get(i);
					
					Neuron node = clone.neuronsMap.get(gene.output.neuronID);
					
					//If the gene's output neuron doesn't exist, create it
					if(node == null){
						node = clone.createNeuron(clone.layers.get(gene.output.layer.index), gene.output.neuronID);
					}
					
					//Make the connection
					clone.connectNeurons(clone.neuronsMap.get(gene.input.neuronID), node, gene.weight, gene.innovation);
					
				}else{
					//Innovation does not exist in either networks. Ignore
				}
			}
			
			//For any disabled axons, 25% chance of being reenabled (https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume21/stanley04a-html/node3.html)
			for(Axon a : clone.axons){
				if(!a.enabled){
					if(random.nextDouble() < 0.25){
						a.enabled = true;
					}
				}
			}
			
			return clone;
	}

	@Override
	public void selection() {
		Collections.sort(networks);
		//Removes the weakest species after the capacity
		if(networks.size() > this.speciesCapacity){
			networks.subList(speciesCapacity, networks.size()).clear();
		}
	}
	
	/**
	 * Gets the deviation between two networks. Similar networks will have smaller deviations.
	 * @param nn1 The first neural network
	 * @param nn2 The second neural network
	 * @return a value between 0-1 representing the deviation
	 */
	private double getDeviation(NeuralNetwork nn1, NeuralNetwork nn2){
		
		double max = Math.max(nn1.axons.size(), nn2.axons.size());
		
		//Counts how many unique genes are in the system
		int same = 0;
		double deltaWeightSum = 0;
		int unique = 0;
		for(Axon a1 : nn1.axons){
			Axon found = null;
			for(Axon a2 : nn2.axons){
				if(a1.innovation == a2.innovation){
					found = a2;
					break;
				}
			}
			
			if(found == null){
				unique++;
			}else{
				same++;
				deltaWeightSum = Math.abs(a1.weight - found.weight);
			}
			
		}
		
		double w = (same == 0) ? 0 : deltaWeightSum / same;
				
		double dev = 0.1 * w + unique / (max == 0 ? 1 : max);
								
		return dev;
	}
	
	/**
	 * Randomnly chooses a mutation.
	 * @param nn The neural network being mutated.
	 * @return
	 */
	private boolean selectMutation(NeuralNetwork nn){
		double r = random.nextDouble();
		
		if(r < 0.25){
			return this.randomNeuronConnection(nn);
		}else if(r < 0.50){
			return this.randomAxonWeightChange(nn);
		}else if(r < 0.75){
			return this.splitRandomConnection(nn);
		}else{
			return this.randomAxonToggle(nn);
		}
	}
	
	private boolean splitRandomConnection(NeuralNetwork nn){
		
		//Only axons that are separated by two or more layers should be considered.
		ArrayList<Axon> possibilities = new ArrayList<Axon>();
		for(Axon a : nn.axons){
			if(a.output.layer.index - a.input.layer.index > 1 && a.enabled){
				possibilities.add(a);
			}
		}
		
		if(possibilities.size() == 0){
			return false; //No possibilities
		}
		
		
		Axon a = possibilities.get(random.nextInt(possibilities.size()));
		
		//Disable this connection, as it's about to be split. We do not remove it however, as it may mutate later.
		a.enabled = false; 
				
		//Create neuron directly in the layer between both (e.g input->layer 2 output->layer 6 will place the neuron in (6-2) / 2 + 2 = 4 after integer division)
		Neuron neuron = nn.createNeuron(nn.layers.get((a.output.layer.index - a.input.layer.index) / 2 + a.input.layer.index), neuronCount++);
		
		//The connection from the input starts off with weight 1 (this actually keeps the functionality of the network identical, until there is a mutation).
		nn.connectNeurons(a.input, neuron, 1, innovationCount++);
				
		//The connection to the output maintains the weight of the original connection
		nn.connectNeurons(neuron, a.output, a.weight, innovationCount++);
				
		return true;
		
	}
	
	private boolean randomAxonToggle(NeuralNetwork nn){
		if(nn.axons.size() == 0)
			return false;
		Axon a = nn.axons.get(random.nextInt(nn.axons.size()));
		a.enabled = !a.enabled;
		return true;
	}

	private boolean randomNeuronConnection(NeuralNetwork nn){	
		
		//Get the non-empty layers from the array
		ArrayList<Layer> possibilities = new ArrayList<Layer>();
		for(Layer l : nn.layers){
			if(l.neurons.size() > 0){
				possibilities.add(l);
			}
		}
		
		//There are not two layers available to create a random neuron connection
		if(possibilities.size() < 2){
			return false;
		}
		
		//Get a split index
		int splitIndex = random.nextInt(possibilities.size() - 1);
		
		//Get neurons before the split
		ArrayList<Neuron> left = new ArrayList<Neuron>();
		for(int i = 0; i <= splitIndex; i++){
			for(Neuron n : possibilities.get(i).neurons){
				left.add(n);
			}
		}
		
		//Get neurons after the split
		ArrayList<Neuron> right = new ArrayList<Neuron>();
		for(int i = splitIndex + 1; i < possibilities.size(); i++){
			for(Neuron n : possibilities.get(i).neurons){
				right.add(n);
			}
		}
		
		Neuron n1 = left.get(random.nextInt(left.size()));
		Neuron n2 = right.get(random.nextInt(right.size()));
		nn.connectNeurons(n1, n2, random.nextDouble() * 2 - 1, innovationCount++);
		
		return true;
		
	}
	
	private boolean randomAxonWeightChange(NeuralNetwork nn){
		if(nn.axons.size() == 0) //No axons to change
			return false;
		Axon a = nn.axons.get(random.nextInt(nn.axons.size()));
		a.weight = random.nextDouble() * 2 - 1;
		return true;
	}
	
	@FunctionalInterface
	public interface SimulateDelegate {

		void simulateNetwork(NeuralNetwork network);
		
	}
	

}
